{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nandwani_vaibhav/anaconda3/envs/fastai/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 59,  62,  63],\n",
       "         [ 43,  46,  45],\n",
       "         [ 50,  48,  43],\n",
       "         ...,\n",
       "         [158, 132, 108],\n",
       "         [152, 125, 102],\n",
       "         [148, 124, 103]],\n",
       "\n",
       "        [[ 16,  20,  20],\n",
       "         [  0,   0,   0],\n",
       "         [ 18,   8,   0],\n",
       "         ...,\n",
       "         [123,  88,  55],\n",
       "         [119,  83,  50],\n",
       "         [122,  87,  57]],\n",
       "\n",
       "        [[ 25,  24,  21],\n",
       "         [ 16,   7,   0],\n",
       "         [ 49,  27,   8],\n",
       "         ...,\n",
       "         [118,  84,  50],\n",
       "         [120,  84,  50],\n",
       "         [109,  73,  42]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[208, 170,  96],\n",
       "         [201, 153,  34],\n",
       "         [198, 161,  26],\n",
       "         ...,\n",
       "         [160, 133,  70],\n",
       "         [ 56,  31,   7],\n",
       "         [ 53,  34,  20]],\n",
       "\n",
       "        [[180, 139,  96],\n",
       "         [173, 123,  42],\n",
       "         [186, 144,  30],\n",
       "         ...,\n",
       "         [184, 148,  94],\n",
       "         [ 97,  62,  34],\n",
       "         [ 83,  53,  34]],\n",
       "\n",
       "        [[177, 144, 116],\n",
       "         [168, 129,  94],\n",
       "         [179, 142,  87],\n",
       "         ...,\n",
       "         [216, 184, 140],\n",
       "         [151, 118,  84],\n",
       "         [123,  92,  72]]],\n",
       "\n",
       "\n",
       "       [[[154, 177, 187],\n",
       "         [126, 137, 136],\n",
       "         [105, 104,  95],\n",
       "         ...,\n",
       "         [ 91,  95,  71],\n",
       "         [ 87,  90,  71],\n",
       "         [ 79,  81,  70]],\n",
       "\n",
       "        [[140, 160, 169],\n",
       "         [145, 153, 154],\n",
       "         [125, 125, 118],\n",
       "         ...,\n",
       "         [ 96,  99,  78],\n",
       "         [ 77,  80,  62],\n",
       "         [ 71,  73,  61]],\n",
       "\n",
       "        [[140, 155, 164],\n",
       "         [139, 146, 149],\n",
       "         [115, 115, 112],\n",
       "         ...,\n",
       "         [ 79,  82,  64],\n",
       "         [ 68,  70,  55],\n",
       "         [ 67,  69,  55]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[175, 167, 166],\n",
       "         [156, 154, 160],\n",
       "         [154, 160, 170],\n",
       "         ...,\n",
       "         [ 42,  34,  36],\n",
       "         [ 61,  53,  57],\n",
       "         [ 93,  83,  91]],\n",
       "\n",
       "        [[165, 154, 128],\n",
       "         [156, 152, 130],\n",
       "         [159, 161, 142],\n",
       "         ...,\n",
       "         [103,  93,  96],\n",
       "         [123, 114, 120],\n",
       "         [131, 121, 131]],\n",
       "\n",
       "        [[163, 148, 120],\n",
       "         [158, 148, 122],\n",
       "         [163, 156, 133],\n",
       "         ...,\n",
       "         [143, 133, 139],\n",
       "         [143, 134, 142],\n",
       "         [143, 133, 144]]],\n",
       "\n",
       "\n",
       "       [[[255, 255, 255],\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253],\n",
       "         ...,\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253],\n",
       "         [253, 253, 253]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         ...,\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255],\n",
       "         [255, 255, 255]],\n",
       "\n",
       "        [[255, 255, 255],\n",
       "         [254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         ...,\n",
       "         [254, 254, 254],\n",
       "         [254, 254, 254],\n",
       "         [254, 254, 254]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[113, 120, 112],\n",
       "         [111, 118, 111],\n",
       "         [105, 112, 106],\n",
       "         ...,\n",
       "         [ 72,  81,  80],\n",
       "         [ 72,  80,  79],\n",
       "         [ 72,  80,  79]],\n",
       "\n",
       "        [[111, 118, 110],\n",
       "         [104, 111, 104],\n",
       "         [ 99, 106,  98],\n",
       "         ...,\n",
       "         [ 68,  75,  73],\n",
       "         [ 70,  76,  75],\n",
       "         [ 78,  84,  82]],\n",
       "\n",
       "        [[106, 113, 105],\n",
       "         [ 99, 106,  98],\n",
       "         [ 95, 102,  94],\n",
       "         ...,\n",
       "         [ 78,  85,  83],\n",
       "         [ 79,  85,  83],\n",
       "         [ 80,  86,  84]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 28,  35,  39],\n",
       "         [ 30,  34,  44],\n",
       "         [ 33,  44,  47],\n",
       "         ...,\n",
       "         [ 43,  56,  45],\n",
       "         [ 52,  64,  53],\n",
       "         [ 46,  58,  47]],\n",
       "\n",
       "        [[ 27,  30,  38],\n",
       "         [ 27,  28,  41],\n",
       "         [ 21,  31,  39],\n",
       "         ...,\n",
       "         [112, 136,  97],\n",
       "         [117, 140, 101],\n",
       "         [115, 138, 100]],\n",
       "\n",
       "        [[ 34,  36,  42],\n",
       "         [ 33,  33,  43],\n",
       "         [ 24,  30,  40],\n",
       "         ...,\n",
       "         [175, 208, 143],\n",
       "         [177, 209, 144],\n",
       "         [176, 208, 143]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[142, 176, 118],\n",
       "         [142, 176, 118],\n",
       "         [150, 184, 127],\n",
       "         ...,\n",
       "         [134, 175, 119],\n",
       "         [128, 168, 112],\n",
       "         [134, 175, 119]],\n",
       "\n",
       "        [[140, 176, 124],\n",
       "         [145, 180, 129],\n",
       "         [150, 186, 134],\n",
       "         ...,\n",
       "         [131, 170, 119],\n",
       "         [130, 170, 119],\n",
       "         [122, 162, 111]],\n",
       "\n",
       "        [[134, 171, 123],\n",
       "         [136, 171, 124],\n",
       "         [136, 171, 124],\n",
       "         ...,\n",
       "         [106, 144, 100],\n",
       "         [104, 142,  99],\n",
       "         [101, 140,  96]]],\n",
       "\n",
       "\n",
       "       [[[134, 186, 223],\n",
       "         [131, 184, 220],\n",
       "         [128, 182, 218],\n",
       "         ...,\n",
       "         [127, 181, 222],\n",
       "         [127, 181, 222],\n",
       "         [128, 182, 223]],\n",
       "\n",
       "        [[133, 189, 228],\n",
       "         [129, 186, 224],\n",
       "         [128, 186, 224],\n",
       "         ...,\n",
       "         [127, 183, 224],\n",
       "         [127, 183, 224],\n",
       "         [128, 184, 225]],\n",
       "\n",
       "        [[128, 185, 226],\n",
       "         [127, 182, 223],\n",
       "         [128, 182, 223],\n",
       "         ...,\n",
       "         [126, 181, 222],\n",
       "         [126, 181, 222],\n",
       "         [126, 180, 221]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[174, 208, 235],\n",
       "         [171, 206, 229],\n",
       "         [155, 189, 216],\n",
       "         ...,\n",
       "         [ 27,  94, 136],\n",
       "         [ 29,  96, 137],\n",
       "         [ 28,  94, 136]],\n",
       "\n",
       "        [[194, 221, 244],\n",
       "         [189, 215, 239],\n",
       "         [159, 196, 225],\n",
       "         ...,\n",
       "         [ 30,  95, 138],\n",
       "         [ 30,  96, 139],\n",
       "         [ 30,  95, 140]],\n",
       "\n",
       "        [[193, 217, 237],\n",
       "         [181, 208, 230],\n",
       "         [168, 201, 227],\n",
       "         ...,\n",
       "         [ 31,  94, 136],\n",
       "         [ 32,  94, 137],\n",
       "         [ 32,  94, 138]]],\n",
       "\n",
       "\n",
       "       [[[125, 125, 116],\n",
       "         [110, 101,  91],\n",
       "         [102,  90,  83],\n",
       "         ...,\n",
       "         [202, 207, 214],\n",
       "         [200, 205, 212],\n",
       "         [202, 208, 214]],\n",
       "\n",
       "        [[142, 146, 142],\n",
       "         [146, 144, 139],\n",
       "         [176, 172, 170],\n",
       "         ...,\n",
       "         [195, 201, 205],\n",
       "         [198, 205, 209],\n",
       "         [204, 211, 215]],\n",
       "\n",
       "        [[180, 185, 183],\n",
       "         [143, 146, 146],\n",
       "         [156, 157, 157],\n",
       "         ...,\n",
       "         [122, 111, 113],\n",
       "         [139, 128, 131],\n",
       "         [158, 147, 150]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[104,  82,  41],\n",
       "         [101,  80,  39],\n",
       "         [101,  81,  38],\n",
       "         ...,\n",
       "         [126, 103,  67],\n",
       "         [126, 103,  69],\n",
       "         [125, 101,  68]],\n",
       "\n",
       "        [[104,  81,  40],\n",
       "         [105,  84,  41],\n",
       "         [109,  88,  43],\n",
       "         ...,\n",
       "         [138, 113,  78],\n",
       "         [137, 113,  80],\n",
       "         [137, 112,  81]],\n",
       "\n",
       "        [[105,  83,  42],\n",
       "         [108,  87,  45],\n",
       "         [115,  94,  50],\n",
       "         ...,\n",
       "         [143, 117,  82],\n",
       "         [143, 116,  84],\n",
       "         [144, 116,  86]]]], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "x_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot Encoding\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Prototype of Model Used :\n",
    "conv-->relu-->conv-->relu-->conv-->relu-->dense-->relu-->dense-->softmax\n",
    "\n",
    "Firstly i used valid convolution with no max pooling as image size was very small. But it was n't working so well. So i changed the convolution to same and used max pool to reduce the size , to obtain much higher accuracy. Adding batchnorm increased the accuracy by a lot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model=Sequential()\n",
    "model.add(Conv2D(32,(3,3),input_shape=(32,32,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(128,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "#model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',input_shape=(32,32,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3),padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt=keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9)\n",
    "opt=keras.optimizers.rmsprop(lr=0.001, decay=1e-4)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "model=load_model('cifar10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/75\n",
      "50000/50000 [==============================] - 23s 460us/step - loss: 1.5735 - acc: 0.4437 - val_loss: 1.6956 - val_acc: 0.4363\n",
      "Epoch 2/75\n",
      "50000/50000 [==============================] - 21s 413us/step - loss: 1.1759 - acc: 0.5952 - val_loss: 1.0263 - val_acc: 0.6511\n",
      "Epoch 3/75\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 1.0359 - acc: 0.6503 - val_loss: 1.0057 - val_acc: 0.6778\n",
      "Epoch 4/75\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.9611 - acc: 0.6774 - val_loss: 0.9469 - val_acc: 0.6990\n",
      "Epoch 5/75\n",
      "50000/50000 [==============================] - 21s 413us/step - loss: 0.9119 - acc: 0.6949 - val_loss: 0.8481 - val_acc: 0.7268\n",
      "Epoch 6/75\n",
      "50000/50000 [==============================] - 21s 416us/step - loss: 0.8627 - acc: 0.7110 - val_loss: 0.7795 - val_acc: 0.7480\n",
      "Epoch 7/75\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.8281 - acc: 0.7246 - val_loss: 0.8835 - val_acc: 0.7092\n",
      "Epoch 8/75\n",
      "50000/50000 [==============================] - 21s 413us/step - loss: 0.7937 - acc: 0.7323 - val_loss: 0.8884 - val_acc: 0.7242\n",
      "Epoch 9/75\n",
      "50000/50000 [==============================] - 21s 413us/step - loss: 0.7655 - acc: 0.7430 - val_loss: 0.7646 - val_acc: 0.7481\n",
      "Epoch 10/75\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.7326 - acc: 0.7527 - val_loss: 0.6919 - val_acc: 0.7683\n",
      "Epoch 11/75\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.7140 - acc: 0.7567 - val_loss: 0.7701 - val_acc: 0.7483\n",
      "Epoch 12/75\n",
      "50000/50000 [==============================] - 21s 412us/step - loss: 0.6829 - acc: 0.7709 - val_loss: 0.7092 - val_acc: 0.7667\n",
      "Epoch 13/75\n",
      "50000/50000 [==============================] - 21s 413us/step - loss: 0.6599 - acc: 0.7757 - val_loss: 0.9014 - val_acc: 0.7044\n",
      "Epoch 14/75\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.6372 - acc: 0.7839 - val_loss: 0.6333 - val_acc: 0.7938\n",
      "Epoch 15/75\n",
      "50000/50000 [==============================] - 21s 412us/step - loss: 0.6267 - acc: 0.7862 - val_loss: 0.7084 - val_acc: 0.7741\n",
      "Epoch 16/75\n",
      "50000/50000 [==============================] - 21s 413us/step - loss: 0.6041 - acc: 0.7956 - val_loss: 0.6267 - val_acc: 0.7976\n",
      "Epoch 17/75\n",
      "50000/50000 [==============================] - 21s 417us/step - loss: 0.5896 - acc: 0.8012 - val_loss: 0.6398 - val_acc: 0.7810\n",
      "Epoch 18/75\n",
      "50000/50000 [==============================] - 21s 413us/step - loss: 0.5652 - acc: 0.8061 - val_loss: 0.6753 - val_acc: 0.7808\n",
      "Epoch 19/75\n",
      "50000/50000 [==============================] - 21s 412us/step - loss: 0.5564 - acc: 0.8092 - val_loss: 0.7264 - val_acc: 0.7666\n",
      "Epoch 20/75\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.5368 - acc: 0.8152 - val_loss: 0.5812 - val_acc: 0.8131\n",
      "Epoch 21/75\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.5186 - acc: 0.8223 - val_loss: 0.6799 - val_acc: 0.7846\n",
      "Epoch 22/75\n",
      "50000/50000 [==============================] - 21s 413us/step - loss: 0.5098 - acc: 0.8246 - val_loss: 0.5549 - val_acc: 0.8183\n",
      "Epoch 23/75\n",
      "50000/50000 [==============================] - 21s 412us/step - loss: 0.5114 - acc: 0.8261 - val_loss: 0.6147 - val_acc: 0.8087\n",
      "Epoch 24/75\n",
      "50000/50000 [==============================] - 21s 413us/step - loss: 0.4933 - acc: 0.8289 - val_loss: 0.6184 - val_acc: 0.8052\n",
      "Epoch 25/75\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.4833 - acc: 0.8327 - val_loss: 0.5887 - val_acc: 0.8086\n",
      "Epoch 26/75\n",
      "50000/50000 [==============================] - 21s 415us/step - loss: 0.4660 - acc: 0.8406 - val_loss: 0.5636 - val_acc: 0.8181\n",
      "Epoch 27/75\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.4635 - acc: 0.8395 - val_loss: 0.5405 - val_acc: 0.8248\n",
      "Epoch 28/75\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.4611 - acc: 0.8429 - val_loss: 0.5412 - val_acc: 0.8212\n",
      "Epoch 29/75\n",
      "50000/50000 [==============================] - 21s 413us/step - loss: 0.4423 - acc: 0.8484 - val_loss: 0.5854 - val_acc: 0.8101\n",
      "Epoch 30/75\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.4364 - acc: 0.8486 - val_loss: 0.5561 - val_acc: 0.8259\n",
      "Epoch 31/75\n",
      "50000/50000 [==============================] - 21s 413us/step - loss: 0.4332 - acc: 0.8516 - val_loss: 0.5654 - val_acc: 0.8232\n",
      "Epoch 32/75\n",
      "50000/50000 [==============================] - 21s 413us/step - loss: 0.4221 - acc: 0.8549 - val_loss: 0.5258 - val_acc: 0.8368\n",
      "Epoch 33/75\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.4157 - acc: 0.8562 - val_loss: 0.5313 - val_acc: 0.8307\n",
      "Epoch 34/75\n",
      "50000/50000 [==============================] - 21s 415us/step - loss: 0.4083 - acc: 0.8575 - val_loss: 0.5053 - val_acc: 0.8413\n",
      "Epoch 35/75\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.4036 - acc: 0.8616 - val_loss: 0.5033 - val_acc: 0.8393\n",
      "Epoch 36/75\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.4005 - acc: 0.8616 - val_loss: 0.5348 - val_acc: 0.8361\n",
      "Epoch 37/75\n",
      "50000/50000 [==============================] - 21s 412us/step - loss: 0.3929 - acc: 0.8652 - val_loss: 0.5020 - val_acc: 0.8374\n",
      "Epoch 38/75\n",
      "50000/50000 [==============================] - 21s 411us/step - loss: 0.3848 - acc: 0.8662 - val_loss: 0.5640 - val_acc: 0.8296\n",
      "Epoch 39/75\n",
      "50000/50000 [==============================] - 21s 416us/step - loss: 0.3828 - acc: 0.8678 - val_loss: 0.5122 - val_acc: 0.8389\n",
      "Epoch 40/75\n",
      "50000/50000 [==============================] - 21s 416us/step - loss: 0.3782 - acc: 0.8700 - val_loss: 0.5190 - val_acc: 0.8363\n",
      "Epoch 41/75\n",
      "50000/50000 [==============================] - 21s 415us/step - loss: 0.3702 - acc: 0.8717 - val_loss: 0.5167 - val_acc: 0.8386\n",
      "Epoch 42/75\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.3630 - acc: 0.8734 - val_loss: 0.5098 - val_acc: 0.8389\n",
      "Epoch 43/75\n",
      "50000/50000 [==============================] - 21s 416us/step - loss: 0.3673 - acc: 0.8730 - val_loss: 0.4865 - val_acc: 0.8436\n",
      "Epoch 44/75\n",
      "50000/50000 [==============================] - 21s 418us/step - loss: 0.3625 - acc: 0.8750 - val_loss: 0.5141 - val_acc: 0.8385\n",
      "Epoch 45/75\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.3544 - acc: 0.8767 - val_loss: 0.5462 - val_acc: 0.8348\n",
      "Epoch 46/75\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.3513 - acc: 0.8775 - val_loss: 0.5355 - val_acc: 0.8367\n",
      "Epoch 47/75\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.3493 - acc: 0.8793 - val_loss: 0.5146 - val_acc: 0.8415\n",
      "Epoch 48/75\n",
      "50000/50000 [==============================] - 21s 413us/step - loss: 0.3403 - acc: 0.8815 - val_loss: 0.5310 - val_acc: 0.8411\n",
      "Epoch 49/75\n",
      "50000/50000 [==============================] - 21s 415us/step - loss: 0.3436 - acc: 0.8800 - val_loss: 0.4996 - val_acc: 0.8427\n",
      "Epoch 50/75\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.3374 - acc: 0.8825 - val_loss: 0.5155 - val_acc: 0.8407\n",
      "Epoch 51/75\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.3352 - acc: 0.8847 - val_loss: 0.5245 - val_acc: 0.8376\n",
      "Epoch 52/75\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.3290 - acc: 0.8856 - val_loss: 0.4922 - val_acc: 0.8471\n",
      "Epoch 53/75\n",
      "50000/50000 [==============================] - 21s 412us/step - loss: 0.3272 - acc: 0.8878 - val_loss: 0.4839 - val_acc: 0.8474\n",
      "Epoch 54/75\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.3278 - acc: 0.8872 - val_loss: 0.4976 - val_acc: 0.8464\n",
      "Epoch 55/75\n",
      "50000/50000 [==============================] - 21s 415us/step - loss: 0.3255 - acc: 0.8879 - val_loss: 0.5081 - val_acc: 0.8467\n",
      "Epoch 56/75\n",
      "50000/50000 [==============================] - 21s 417us/step - loss: 0.3187 - acc: 0.8918 - val_loss: 0.4990 - val_acc: 0.8488\n",
      "Epoch 57/75\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.3159 - acc: 0.8918 - val_loss: 0.5061 - val_acc: 0.8410\n",
      "Epoch 58/75\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.3126 - acc: 0.8916 - val_loss: 0.4834 - val_acc: 0.8497\n",
      "Epoch 59/75\n",
      "50000/50000 [==============================] - 21s 415us/step - loss: 0.3112 - acc: 0.8919 - val_loss: 0.4793 - val_acc: 0.8524\n",
      "Epoch 60/75\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.3101 - acc: 0.8921 - val_loss: 0.4880 - val_acc: 0.8470\n",
      "Epoch 61/75\n",
      "50000/50000 [==============================] - 21s 415us/step - loss: 0.3101 - acc: 0.8918 - val_loss: 0.4743 - val_acc: 0.8501\n",
      "Epoch 62/75\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.3034 - acc: 0.8952 - val_loss: 0.5145 - val_acc: 0.8440\n",
      "Epoch 63/75\n",
      "50000/50000 [==============================] - 21s 417us/step - loss: 0.3031 - acc: 0.8955 - val_loss: 0.5263 - val_acc: 0.8424\n",
      "Epoch 64/75\n",
      "50000/50000 [==============================] - 21s 416us/step - loss: 0.3006 - acc: 0.8948 - val_loss: 0.4955 - val_acc: 0.8476\n",
      "Epoch 65/75\n",
      "50000/50000 [==============================] - 21s 413us/step - loss: 0.3003 - acc: 0.8977 - val_loss: 0.4982 - val_acc: 0.8460\n",
      "Epoch 66/75\n",
      "50000/50000 [==============================] - 21s 413us/step - loss: 0.2995 - acc: 0.8974 - val_loss: 0.5089 - val_acc: 0.8446\n",
      "Epoch 67/75\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.2937 - acc: 0.8982 - val_loss: 0.4909 - val_acc: 0.8497\n",
      "Epoch 68/75\n",
      "50000/50000 [==============================] - 21s 412us/step - loss: 0.2937 - acc: 0.8978 - val_loss: 0.4984 - val_acc: 0.8511\n",
      "Epoch 69/75\n",
      "50000/50000 [==============================] - 21s 413us/step - loss: 0.2879 - acc: 0.8995 - val_loss: 0.5071 - val_acc: 0.8496\n",
      "Epoch 70/75\n",
      "50000/50000 [==============================] - 21s 414us/step - loss: 0.2863 - acc: 0.9020 - val_loss: 0.4919 - val_acc: 0.8508\n",
      "Epoch 71/75\n",
      "50000/50000 [==============================] - 21s 415us/step - loss: 0.2871 - acc: 0.9012 - val_loss: 0.5003 - val_acc: 0.8532\n",
      "Epoch 72/75\n",
      "50000/50000 [==============================] - 21s 413us/step - loss: 0.2834 - acc: 0.9014 - val_loss: 0.4923 - val_acc: 0.8532\n",
      "Epoch 73/75\n",
      "50000/50000 [==============================] - 21s 411us/step - loss: 0.2802 - acc: 0.9027 - val_loss: 0.4912 - val_acc: 0.8528\n",
      "Epoch 74/75\n",
      "50000/50000 [==============================] - 20s 409us/step - loss: 0.2811 - acc: 0.9030 - val_loss: 0.4988 - val_acc: 0.8501\n",
      "Epoch 75/75\n",
      "50000/50000 [==============================] - 21s 412us/step - loss: 0.2775 - acc: 0.9031 - val_loss: 0.4878 - val_acc: 0.8498\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data=model.fit(x_train, y_train,\n",
    "              batch_size=64,\n",
    "              epochs=75,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting loss function with number of epochs\n",
    "plt.plot(data.history['loss'])\n",
    "plt.plot(data.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "#As the test loss became constant, and training loss is still decreasing the function is still overfitting a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cifar10.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "\n",
    " I tried some data augmentation too , but  it did not work so well, so i haven't trained it completely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "782/782 [==============================] - 28s 36ms/step - loss: 2.1516 - acc: 0.1900 - val_loss: 13.8922 - val_acc: 0.1381\n",
      "Epoch 2/25\n",
      "782/782 [==============================] - 28s 36ms/step - loss: 2.1078 - acc: 0.2093 - val_loss: 13.8942 - val_acc: 0.1379\n",
      "Epoch 3/25\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 2.0840 - acc: 0.2244 - val_loss: 13.9083 - val_acc: 0.1369\n",
      "Epoch 4/25\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 2.0686 - acc: 0.2318 - val_loss: 14.0694 - val_acc: 0.1270\n",
      "Epoch 5/25\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 2.0531 - acc: 0.2373 - val_loss: 14.2293 - val_acc: 0.1170\n",
      "Epoch 6/25\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 2.0383 - acc: 0.2466 - val_loss: 14.3999 - val_acc: 0.1066\n",
      "Epoch 7/25\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 1.9327 - acc: 0.2846 - val_loss: 14.1383 - val_acc: 0.1226\n",
      "Epoch 8/25\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 1.8540 - acc: 0.3212 - val_loss: 14.0685 - val_acc: 0.1268\n",
      "Epoch 9/25\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 1.8110 - acc: 0.3397 - val_loss: 14.3491 - val_acc: 0.1096\n",
      "Epoch 10/25\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 1.7687 - acc: 0.3571 - val_loss: 14.4153 - val_acc: 0.1055\n",
      "Epoch 11/25\n",
      "742/782 [===========================>..] - ETA: 1s - loss: 1.7292 - acc: 0.3717"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-23052c8020c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                         workers=4)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    212\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=64),\n",
    "                        epochs=25,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
